{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime, timedelta, date\n",
    "from functools import reduce\n",
    "from io import BytesIO\n",
    "import findspark\n",
    "\n",
    "from config import SPARK_APP_NAME, SPARK_MASTER, MINIO_ACCESS_KEY, MINIO_SECRET_KEY, MINIO_ENDPOINT\n",
    "\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 20:09:00 WARN Utils: Your hostname, maxp-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/04/28 20:09:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/maxp/Work/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/maxp/Work/spark-3.1.1-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/maxp/.ivy2/cache\n",
      "The jars for the packages stored in: /home/maxp/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0dbc7852-5e49-4952-aa8f-84f13845b8bf;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.postgresql#postgresql;42.7.5 in central\n",
      "\tfound org.checkerframework#checker-qual;3.48.3 in central\n",
      ":: resolution report :: resolve 1112ms :: artifacts dl 46ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.48.3 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.5 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0dbc7852-5e49-4952-aa8f-84f13845b8bf\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/60ms)\n",
      "25/04/28 20:09:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_230964/1675531981.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.hadoop.fs.s3a.aws.credentials.provider\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.jars.packages\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.7.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                     getattr(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[1;32m    587\u001b[0m                 )\n\u001b[1;32m    588\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0mjsparkSession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             getattr(getattr(self._jvm, \"SparkSession$\"), \"MODULE$\").applyModifiableSettings(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1587\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1588\u001b[0m             answer, self._gateway_client, None, self._fqn)\n\u001b[1;32m   1589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 raise Py4JError(\n\u001b[0m\u001b[1;32m    331\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(SPARK_APP_NAME) \\\n",
    "    .master(SPARK_MASTER) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.7.5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'access_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_232869/3543490259.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.hadoop.fs.s3a.endpoint\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http://localhost:9000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.hadoop.fs.s3a.access.key\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.hadoop.fs.s3a.secret.key\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecret_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.hadoop.fs.s3a.path.style.access\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'access_key' is not defined"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HTMLProcessor\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.7.5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_city_name(city):\n",
    "        if not city:\n",
    "            return None\n",
    "        \n",
    "        city = re.sub(r\"(^[\\s\\-]+)|([\\s\\-]+$)\", \"\", city)\n",
    "        city = re.sub(r\"\\s+\", \" \", city)\n",
    "        city = re.sub(r\"[^А-ЯЁа-яё\\-\\s]\", \"\", city)\n",
    "        \n",
    "        if city:\n",
    "            city = city.strip()\n",
    "            parts = [p.capitalize() for p in city.split()]\n",
    "            city = \" \".join(parts)\n",
    "            city = re.sub(r\"\\bИ\\b\", \"и\", city)\n",
    "\n",
    "        city = city if city and len(city) >= 3 else None\n",
    "        return city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNFSO_html_processor_spark:\n",
    "    \n",
    "    def __init__(self, url_date, html_page, spark):\n",
    "        self.url_date = url_date\n",
    "        self.soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "        self.spark = spark\n",
    "        self.relay_dates_list = []\n",
    "    \n",
    "    def get_date_str(self):\n",
    "        year = self.url_date[:4]\n",
    "        month = self.url_date[4:6]\n",
    "        day = self.url_date[6:8]\n",
    "        return f\"{day}.{month}.{year}\"\n",
    "    \n",
    "    def get_date(self):\n",
    "        try:\n",
    "            year = int(self.url_date[:4])\n",
    "            month = int(self.url_date[4:6])\n",
    "            day = int(self.url_date[6:8])\n",
    "            return date(year, month, day)\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f\"Ошибка преобразования даты: {e}\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "\n",
    "    def parse_events(self):\n",
    "        if not self.soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            date_str = self.get_date_str()\n",
    "            date = self.get_date()\n",
    "            title_text = \" \".join(self.soup.find(\"h1\").stripped_strings)\n",
    "            event_name = clean_text(title_text.split(date_str)[0]) if date_str else clean_text(title_text)\n",
    "\n",
    "            if \"эстафета\" in event_name.lower():\n",
    "                self.relay_dates_list.append(date)\n",
    "                # print(\"Пропуск эстафеты\")\n",
    "                return\n",
    "\n",
    "            city_matches = re.finditer(r\"(?i)г\\.?\\s*([А-ЯЁ]{1}[а-яё\\-\\s]+)(?=\\s|$|,|\\.|<|\\))\", title_text)\n",
    "        \n",
    "            cities = []\n",
    "            for match in city_matches:\n",
    "                city = clean_city_name(match.group(1))\n",
    "                if city and len(city) >= 3 and not any(c.isdigit() for c in city):\n",
    "                    cities.append(city)\n",
    "            \n",
    "            city = cities[-1].split()[0] if cities else None\n",
    "\n",
    "            schema = StructType([\n",
    "                StructField(\"start_name\", StringType(), True),\n",
    "                StructField(\"date\", DateType(), True),\n",
    "                StructField(\"city\", StringType(), True)\n",
    "            ])\n",
    "            \n",
    "            data = [(event_name, date, city)]\n",
    "            df_events = self.spark.createDataFrame(data, schema)\n",
    "            return df_events\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"parse_events error: \", e)\n",
    "            return None\n",
    "    \n",
    "    def parse_distances(self):\n",
    "        if not self.soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            date_str = self.get_date_str()\n",
    "            date = self.get_date()\n",
    "            distances = []\n",
    "\n",
    "            if date in self.relay_dates_list:\n",
    "                return\n",
    "            \n",
    "            for h2 in self.soup.find_all(\"h2\"):\n",
    "                text = h2.get_text()\n",
    "                match = re.match(r\"(.+),\\s*(\\d+)\\s*КП,\\s*([\\d.,]+)\\s*(км|м)\", text)\n",
    "                if match:\n",
    "                    group, kp, length, unit = match.groups()\n",
    "                    length = float(length.replace(\",\", \".\"))\n",
    "                    # конвертируем метры в километры\n",
    "                    if unit == \"м\":\n",
    "                        length /= 1000\n",
    "                    distances.append((date, group, int(kp), round(length, 2)))\n",
    "\n",
    "            schema = StructType([\n",
    "                StructField(\"date\", DateType(), True),\n",
    "                StructField(\"group\", StringType(), True),\n",
    "                StructField(\"cp\", IntegerType(), True),\n",
    "                StructField(\"length\", FloatType(), True)\n",
    "            ])\n",
    "            \n",
    "            df_distances = self.spark.createDataFrame(distances, schema)\n",
    "            return df_distances\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"parse_distances error: \", e)\n",
    "            return None\n",
    "    \n",
    "    def parse_results(self):\n",
    "        if not self.soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            date = self.get_date()\n",
    "            categories = self.soup.find_all('h2')\n",
    "            results_data = []\n",
    "            \n",
    "            if date in self.relay_dates_list:\n",
    "                return\n",
    "\n",
    "            for category in categories:\n",
    "                group = category.get_text(strip=True).split(',')[0]\n",
    "                pre_tag = category.find_next('pre')\n",
    "                \n",
    "                if not pre_tag:\n",
    "                    continue\n",
    "                \n",
    "                lines = pre_tag.get_text().split('\\n')\n",
    "                \n",
    "                for line in lines:\n",
    "                    fields = self.results_regex_parser(line)\n",
    "                    if fields:\n",
    "                        place = int(fields[8].replace('=', '').strip())\n",
    "                        results_data.append((\n",
    "                            date, \n",
    "                            group, \n",
    "                            int(fields[0]), \n",
    "                            fields[1], \n",
    "                            fields[2], \n",
    "                            fields[3],\n",
    "                            int(fields[4]), \n",
    "                            int(fields[5]), \n",
    "                            fields[6], \n",
    "                            fields[7], \n",
    "                            place\n",
    "                        ))\n",
    "\n",
    "            schema = StructType([\n",
    "                StructField(\"date\", DateType(), True),\n",
    "                StructField(\"group\", StringType(), True),\n",
    "                StructField(\"position_number\", IntegerType(), True),\n",
    "                StructField(\"full_name\", StringType(), True),\n",
    "                StructField(\"team\", StringType(), True),\n",
    "                StructField(\"qualification\", StringType(), True),\n",
    "                StructField(\"number\", IntegerType(), True),\n",
    "                StructField(\"year_of_birth\", IntegerType(), True),\n",
    "                StructField(\"result_time\", StringType(), True),\n",
    "                StructField(\"time_gap\", StringType(), True),\n",
    "                StructField(\"finish_position\", IntegerType(), True)\n",
    "            ])\n",
    "            \n",
    "            df_results = self.spark.createDataFrame(results_data, schema)\n",
    "            return df_results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"parse_results error: \", e)\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def results_regex_parser(line):\n",
    "        \"\"\"Парсит строку с результатами (без изменений)\"\"\"\n",
    "        pattern = r'''\n",
    "            ^\\s*(\\d+)\\s+                # №п/п\n",
    "            ([А-ЯЁ][а-яё-]+\\s[А-ЯЁ][а-яё-]+)\\s+  # Фамилия и имя\n",
    "            (.*?)\\s{2,}                 # Коллектив\n",
    "            ([А-Яa-zIЮМСК]+)?\\s*        # Квал (может отсутствовать)\n",
    "            (\\d+)\\s+                    # Номер\n",
    "            (\\d{4})\\s+                  # Год рождения\n",
    "            (\\d{2}:\\d{2}:\\d{2})\\s+      # Результат\n",
    "            (\\+\\d{2}:\\d{2})\\s+          # Отставание\n",
    "            (=?\\s*\\d+)\\s*               # Место\n",
    "            (.*)                        # Примечание (если есть)\n",
    "        '''\n",
    "        match = re.search(pattern, line, re.VERBOSE | re.IGNORECASE)\n",
    "        return match.groups() if match else False\n",
    "    \n",
    "    def parse_all(self):\n",
    "        if not self.soup:\n",
    "            return None, None, None\n",
    "        try:\n",
    "            df_events = self.parse_events()\n",
    "            df_distances = self.parse_distances()\n",
    "            df_results = self.parse_results()\n",
    "            return df_events, df_distances, df_results\n",
    "        except Exception as e:\n",
    "            print(\"parse_all error: \", e)\n",
    "            return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240407', '20240412', '20240413', '20240414', '20240420', '20240421', '20240428', '20240429', '20240430', '20240509', '20240510', '20240511', '20240512', '20240518', '20240521', '20240522', '20240525', '20240526', '20240602', '20240608', '20240609', '20240721', '20240907', '20240908', '20240913', '20240914', '20240915', '20240928', '20240929', '20241006', '20241012', '20241013', '20241019', '20241026', '20241027', '20241102', '20241103', '20241117', '20241201']\n"
     ]
    }
   ],
   "source": [
    "bucket_obj_list = [obj.object_name for obj in client.list_objects(bucket_html)]\n",
    "print(bucket_obj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dfs, dist_dfs, res_dfs = [], [], []\n",
    "\n",
    "for key in bucket_obj_list:\n",
    "    html = client.get_object(bucket_html, key).read().decode('windows-1251')\n",
    "    parser = VRNFSO_html_processor_spark(key, html, spark)\n",
    "    df_ev, df_ds, df_rs = parser.parse_all()\n",
    "    if df_ev is not None:\n",
    "        events_dfs.append(df_ev)\n",
    "    if df_ds is not None: \n",
    "        dist_dfs.append(df_ds)\n",
    "    if df_rs is not None: \n",
    "        res_dfs.append(df_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_all(dfs):\n",
    "    if not dfs:\n",
    "        return spark.createDataFrame([], StructType())\n",
    "    return reduce(DataFrame.unionByName, dfs[1:], dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events    = union_all(events_dfs)\n",
    "all_distances = union_all(dist_dfs)\n",
    "all_results   = union_all(res_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('start_name', StringType(), True), StructField('date', DateType(), True), StructField('city', StringType(), True)])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:=======================================>                (14 + 6) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---+------+\n",
      "|      date|  group| cp|length|\n",
      "+----------+-------+---+------+\n",
      "|2024-04-07|    Ж10| 21|   6.4|\n",
      "|2024-04-07|    Ж12| 21|   6.4|\n",
      "|2024-04-07|    Ж14| 21|   6.4|\n",
      "|2024-04-07|    Ж16| 21|   6.4|\n",
      "|2024-04-07|    Ж20| 21|   6.4|\n",
      "|2024-04-07|    Ж35| 21|   6.4|\n",
      "|2024-04-07|    Ж55| 21|   6.4|\n",
      "|2024-04-07|     ЖЭ| 21|   6.4|\n",
      "|2024-04-07|    М10| 21|   6.4|\n",
      "|2024-04-07|    М12| 21|   6.4|\n",
      "|2024-04-07|    М14| 21|   6.4|\n",
      "|2024-04-07|    М16| 21|   6.4|\n",
      "|2024-04-07|    М20| 21|   6.4|\n",
      "|2024-04-07|    М35| 21|   6.4|\n",
      "|2024-04-07|    М55| 21|   6.4|\n",
      "|2024-04-07|     МЭ| 21|   6.4|\n",
      "|2024-04-07|Новички| 21|   6.4|\n",
      "|2024-04-12|    Ж12| 10|   1.9|\n",
      "|2024-04-12|    Ж14| 13|   2.4|\n",
      "|2024-04-12|    Ж16| 14|   2.9|\n",
      "|2024-04-12|    Ж18| 15|   3.2|\n",
      "|2024-04-12|    Ж20| 17|   3.5|\n",
      "|2024-04-12|    Ж30| 15|   3.2|\n",
      "|2024-04-12|    Ж40| 14|   3.1|\n",
      "|2024-04-12|    Ж50| 14|   2.6|\n",
      "|2024-04-12|    Ж60| 13|   2.4|\n",
      "|2024-04-12|     ЖБ| 14|   3.1|\n",
      "|2024-04-12|Женщины| 17|   3.5|\n",
      "|2024-04-12|    М12| 12|   2.1|\n",
      "|2024-04-12|    М14| 14|   2.6|\n",
      "|2024-04-12|    М16| 15|   3.2|\n",
      "|2024-04-12|    М18| 17|   3.6|\n",
      "|2024-04-12|    М20| 20|   4.0|\n",
      "|2024-04-12|    М30| 17|   3.6|\n",
      "|2024-04-12|    М40| 16|   3.3|\n",
      "|2024-04-12|    М50| 15|   3.2|\n",
      "|2024-04-12|    М60| 14|   3.1|\n",
      "|2024-04-12|     МБ| 16|   3.3|\n",
      "|2024-04-12|Мужчины| 20|   4.0|\n",
      "|2024-04-13|    Ж12|  8|   2.4|\n",
      "|2024-04-13|    Ж14| 12|   4.0|\n",
      "|2024-04-13|    Ж16| 19|   5.6|\n",
      "|2024-04-13|    Ж18| 20|   6.0|\n",
      "|2024-04-13|    Ж20| 24|   9.0|\n",
      "|2024-04-13|    Ж30| 20|   6.0|\n",
      "|2024-04-13|    Ж40| 19|   5.8|\n",
      "|2024-04-13|    Ж50| 14|   4.6|\n",
      "|2024-04-13|    Ж60| 12|   4.0|\n",
      "|2024-04-13|     ЖБ| 19|   5.8|\n",
      "|2024-04-13|Женщины| 24|   9.0|\n",
      "+----------+-------+---+------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_distances.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/28 09:26:18 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_events.write.mode(\"overwrite\").parquet(\"s3a://processed-data/events/\")\n",
    "all_distances.write.mode(\"overwrite\").parquet(\"s3a://processed-data/distances/\")\n",
    "all_results.write.mode(\"overwrite\").parquet(\"s3a://processed-data/results/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_name: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- length: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- position_number: integer (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- team: string (nullable = true)\n",
      " |-- qualification: string (nullable = true)\n",
      " |-- number: integer (nullable = true)\n",
      " |-- year_of_birth: integer (nullable = true)\n",
      " |-- result_time: string (nullable = true)\n",
      " |-- time_gap: string (nullable = true)\n",
      " |-- finish_position: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_events_loaded = spark.read.parquet(\"s3a://processed-data/events/\")\n",
    "sdf_distances_loaded = spark.read.parquet(\"s3a://processed-data/distances/\")\n",
    "sdf_results_loaded = spark.read.parquet(\"s3a://processed-data/results/\")\n",
    "\n",
    "# sdf_events_cleaned = sdf_events_loaded.fillna(\"Воронеж\", subset=[\"Город\"])\n",
    "\n",
    "# sdf_events_cleaned.write.mode(\"overwrite\").parquet(\"s3a://processed-data/events/\")\n",
    "\n",
    "sdf_events_loaded.printSchema()\n",
    "sdf_distances_loaded.printSchema()\n",
    "sdf_results_loaded.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = \"jdbc:postgresql://127.0.0.1:5433/postgres\"\n",
    "\n",
    "db_properties = {\n",
    "    \"user\": postgres_user,\n",
    "    \"password\": postgres_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# sdf_events_loaded.write.mode(\"overwrite\").jdbc(db_url, \"public.events\", properties=db_properties)\n",
    "# sdf_distances_loaded.write.mode(\"overwrite\").jdbc(db_url, \"public.distances\", properties=db_properties)\n",
    "# sdf_results_loaded.write.mode(\"overwrite\").jdbc(db_url, \"public.results\", properties=db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_events = sdf_events_loaded.select(F.substring(F.col(\"start_name\"), 1, 100).alias(\"event_name\"), \n",
    "                                      F.col(\"date\").alias(\"event_date\"), \n",
    "                                      F.substring(F.col(\"city\"), 1, 50).alias(\"city\")).distinct()\n",
    "\n",
    "sdf_groups = sdf_distances_loaded.select(F.col(\"date\").alias(\"event_date\"), \n",
    "                                         F.substring(F.col(\"group\"), 1, 20).alias(\"group_name\"), \n",
    "                                         F.col(\"cp\"), \n",
    "                                         F.col(\"length\").alias(\"length_km\")).distinct()\n",
    "\n",
    "sdf_participants = sdf_results_loaded.select(F.col(\"full_name\"), \n",
    "                                             F.substring(F.col(\"team\"), 1, 50).alias(\"team\"), \n",
    "                                             F.col(\"year_of_birth\").alias(\"birth_year\")).distinct()\n",
    "\n",
    "sdf_results = sdf_results_loaded.select(F.col(\"date\").alias(\"event_date\"), \n",
    "                                        F.col(\"group\").alias(\"group_name\"),\n",
    "                                        F.col(\"position_number\"),\n",
    "                                        F.col(\"full_name\"),\n",
    "                                        F.col(\"team\"),\n",
    "                                        F.substring(F.col(\"qualification\"), 1, 10).alias(\"qualification\"),\n",
    "                                        F.col(\"number\").alias(\"bib_number\"),\n",
    "                                        F.col(\"year_of_birth\").alias(\"birth_year\"),\n",
    "                                        F.col(\"finish_position\"),\n",
    "                                        F.col(\"result_time\"),\n",
    "                                        F.col(\"time_gap\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_events.write.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"events\",\n",
    "    mode=\"append\",\n",
    "    properties=db_properties\n",
    ")\n",
    "\n",
    "events_lookup = spark.read.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"events\",\n",
    "    properties=db_properties\n",
    ").select(\"event_id\", \"event_date\")\n",
    "\n",
    "sdf_groups = sdf_groups.join(\n",
    "    events_lookup,\n",
    "    on=\"event_date\",\n",
    "    how=\"inner\"\n",
    ").select(\"group_name\", \"cp\", \"length_km\", \"event_id\")\n",
    "\n",
    "sdf_groups.write.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"group_params\",\n",
    "    mode=\"append\",\n",
    "    properties=db_properties\n",
    ")\n",
    "\n",
    "groups_lookup = spark.read.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"group_params\",\n",
    "    properties=db_properties\n",
    ").select(\"group_id\", \"event_id\", \"group_name\")\n",
    "\n",
    "sdf_participants.write.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"participants\",\n",
    "    mode=\"append\",\n",
    "    properties=db_properties\n",
    ")\n",
    "\n",
    "participants_lookup = spark.read.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"participants\",\n",
    "    properties=db_properties\n",
    ").select(\"participant_id\", \"full_name\", \"team\", \"birth_year\")\n",
    "\n",
    "sdf_results = sdf_results \\\n",
    "    .join(events_lookup,       on=\"event_date\",             how=\"inner\") \\\n",
    "    .join(groups_lookup,       on=[\"event_id\",\"group_name\"], how=\"inner\") \\\n",
    "    .join(participants_lookup, on=[\"full_name\",\"team\",\"birth_year\"], how=\"inner\") \\\n",
    "    .select(\n",
    "        F.col(\"event_id\"),\n",
    "        F.col(\"group_id\"),\n",
    "        F.col(\"participant_id\"),\n",
    "        F.col(\"position_number\"),\n",
    "        F.col(\"qualification\"),\n",
    "        F.col(\"bib_number\"),\n",
    "        F.col(\"finish_position\"),\n",
    "        F.col(\"result_time\"),\n",
    "        F.col(\"time_gap\")\n",
    "    )\n",
    "\n",
    "sdf_results.write.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"results\",\n",
    "    mode=\"append\",\n",
    "    properties=db_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_events.write.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"events\",\n",
    "    mode=\"append\",\n",
    "    properties=db_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_lookup = spark.read.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"events\",\n",
    "    properties=db_properties\n",
    ").select(\"event_id\", \"event_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_groups = sdf_groups.join(\n",
    "    events_lookup,\n",
    "    on=\"event_date\",\n",
    "    how=\"inner\"\n",
    ").select(\"group_name\", \"cp\", \"length_km\", \"event_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_groups.write.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"group_params\",\n",
    "    mode=\"append\",\n",
    "    properties=db_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_lookup = spark.read.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"group_params\",\n",
    "    properties=db_properties\n",
    ").select(\"group_id\", \"event_id\", \"group_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_participants.write.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"participants\",\n",
    "    mode=\"append\",\n",
    "    properties=db_properties\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_lookup = spark.read.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"participants\",\n",
    "    properties=db_properties\n",
    ").select(\"participant_id\", \"full_name\", \"team\", \"birth_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_results = sdf_results \\\n",
    "    .join(events_lookup,       on=\"event_date\",             how=\"inner\") \\\n",
    "    .join(groups_lookup,       on=[\"event_id\",\"group_name\"], how=\"inner\") \\\n",
    "    .join(participants_lookup, on=[\"full_name\",\"team\",\"birth_year\"], how=\"inner\") \\\n",
    "    .select(\n",
    "        F.col(\"event_id\"),\n",
    "        F.col(\"group_id\"),\n",
    "        F.col(\"participant_id\"),\n",
    "        F.col(\"position_number\"),\n",
    "        F.col(\"qualification\"),\n",
    "        F.col(\"bib_number\"),\n",
    "        F.col(\"finish_position\"),\n",
    "        F.col(\"result_time\"),\n",
    "        F.col(\"time_gap\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_results.write.jdbc(\n",
    "    url=db_url,\n",
    "    table=\"results\",\n",
    "    mode=\"append\",\n",
    "    properties=db_properties\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
