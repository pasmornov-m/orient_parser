{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from minio import Minio\n",
    "from io import BytesIO\n",
    "import findspark\n",
    "\n",
    "\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "access_key = os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "secret_key = os.getenv(\"MINIO_SECRET_KEY\")\n",
    "\n",
    "postgres_user = os.getenv(\"POSTGRES_USER\")\n",
    "postgres_password = os.getenv(\"POSTGRES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Minio(\n",
    "    endpoint=\"localhost:9000\",\n",
    "    access_key=access_key,\n",
    "    secret_key=secret_key,\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "bucket_html =\"raw-html\"\n",
    "bucket_processed = \"processed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HTMLProcessor\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4, org.postgresql:postgresql:42.7.5\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[\\n\\r\\t]+\", \" \", text).strip()\n",
    "    cleaned_text = re.sub(r\" +\", \" \", cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VRNFSO_html_processor_spark:\n",
    "    \n",
    "    def __init__(self, url_date, html_page, spark):\n",
    "        self.url_date = url_date\n",
    "        self.soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "        self.spark = spark\n",
    "    \n",
    "    def get_date(self):\n",
    "        year = self.url_date[:4]\n",
    "        month = self.url_date[4:6]\n",
    "        day = self.url_date[6:8]\n",
    "        return f\"{day}.{month}.{year}\"\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Очистка текста от лишних символов\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        cleaned = re.sub(r\"[\\n\\r\\t]+\", \" \", text)\n",
    "        cleaned = re.sub(r\" +\", \" \", cleaned)\n",
    "        return cleaned.strip()\n",
    "    \n",
    "    def parse_events(self):\n",
    "        \"\"\"Парсит название соревнования, дату и город в Spark DataFrame\"\"\"\n",
    "        if not self.soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            title_text = self.soup.find(\"h1\").get_text(separator=\" \").strip()\n",
    "            date = self.get_date()\n",
    "            parts = title_text.split(date) if date else [title_text]\n",
    "            event_name = self.clean_text(parts[0])\n",
    "\n",
    "            if \"эстафета\" in event_name.lower():\n",
    "                print(\"Пропуск эстафеты\")\n",
    "                return None\n",
    "\n",
    "            city_match = re.search(r\"г\\.\\s*([\\w\\s]+)\\n\", parts[1]) if len(parts) > 1 else None\n",
    "            city = city_match.group(1) if city_match else None\n",
    "\n",
    "            # Создаем Spark DataFrame\n",
    "            schema = StructType([\n",
    "                StructField(\"Название старта\", StringType()),\n",
    "                StructField(\"Дата\", DateType()),\n",
    "                StructField(\"Город\", StringType())\n",
    "            ])\n",
    "            \n",
    "            data = [(event_name, date, city)]\n",
    "            df_events = self.spark.createDataFrame(data, schema)\n",
    "            return df_events\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"parse_events error: \", e)\n",
    "            return None\n",
    "    \n",
    "    def parse_distances(self):\n",
    "        \"\"\"Парсит информацию о дистанциях в Spark DataFrame\"\"\"\n",
    "        if not self.soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            date = self.get_date()\n",
    "            distances = []\n",
    "            \n",
    "            for h2 in self.soup.find_all(\"h2\"):\n",
    "                text = h2.get_text()\n",
    "                match = re.match(r\"(.+),\\s*(\\d+)\\s*КП,\\s*([\\d.,]+)\\s*(км|м)\", text)\n",
    "                if match:\n",
    "                    group, kp, length, unit = match.groups()\n",
    "                    length = float(length.replace(\",\", \".\"))\n",
    "                    if unit == \"м\":\n",
    "                        length /= 1000  # конвертируем метры в километры\n",
    "                    distances.append((date, group, int(kp), length))\n",
    "\n",
    "            # Создаем Spark DataFrame\n",
    "            schema = StructType([\n",
    "                StructField(\"Дата\", DateType()),\n",
    "                StructField(\"Группа\", StringType()),\n",
    "                StructField(\"КП\", IntegerType()),\n",
    "                StructField(\"Длина\", FloatType())\n",
    "            ])\n",
    "            \n",
    "            df_distances = self.spark.createDataFrame(distances, schema)\n",
    "            return df_distances\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"parse_distances error: \", e)\n",
    "            return None\n",
    "    \n",
    "    def parse_results(self):\n",
    "        \"\"\"Парсит результаты соревнований в Spark DataFrame\"\"\"\n",
    "        if not self.soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            date = self.get_date()\n",
    "            categories = self.soup.find_all('h2')\n",
    "            results_data = []\n",
    "            \n",
    "            for category in categories:\n",
    "                group = category.get_text(strip=True).split(',')[0]\n",
    "                pre_tag = category.find_next('pre')\n",
    "                \n",
    "                if not pre_tag:\n",
    "                    continue\n",
    "                \n",
    "                lines = pre_tag.get_text().split('\\n')\n",
    "                \n",
    "                for line in lines:\n",
    "                    fields = self.results_regex_parser(line)\n",
    "                    if fields:\n",
    "                        place = fields[8].replace('=', '').strip()\n",
    "                        results_data.append((\n",
    "                            date, group, int(fields[0]), fields[1], fields[2], fields[3],\n",
    "                            int(fields[4]), int(fields[5]), fields[6], fields[7], int(place)\n",
    "                        ))\n",
    "\n",
    "            # Создаем Spark DataFrame\n",
    "            schema = StructType([\n",
    "                StructField(\"Дата\", DateType()),\n",
    "                StructField(\"Группа\", StringType()),\n",
    "                StructField(\"№п/п\", IntegerType()),\n",
    "                StructField(\"Фамилия, имя\", StringType()),\n",
    "                StructField(\"Коллектив\", StringType()),\n",
    "                StructField(\"Квал\", StringType()),\n",
    "                StructField(\"Номер\", IntegerType()),\n",
    "                StructField(\"ГР\", IntegerType()),\n",
    "                StructField(\"Результат\", TimestampType()),\n",
    "                StructField(\"Отставание\", TimestampType()),\n",
    "                StructField(\"Место\", IntegerType())\n",
    "            ])\n",
    "            \n",
    "            df_results = self.spark.createDataFrame(results_data, schema)\n",
    "            return df_results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"parse_results error: \", e)\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def results_regex_parser(line):\n",
    "        \"\"\"Парсит строку с результатами (без изменений)\"\"\"\n",
    "        pattern = r'''\n",
    "            ^\\s*(\\d+)\\s+                # №п/п\n",
    "            ([А-ЯЁ][а-яё-]+\\s[А-ЯЁ][а-яё-]+)\\s+  # Фамилия и имя\n",
    "            (.*?)\\s{2,}                 # Коллектив\n",
    "            ([А-Яa-zIЮМСК]+)?\\s*        # Квал (может отсутствовать)\n",
    "            (\\d+)\\s+                    # Номер\n",
    "            (\\d{4})\\s+                  # Год рождения\n",
    "            (\\d{2}:\\d{2}:\\d{2})\\s+      # Результат\n",
    "            (\\+\\d{2}:\\d{2})\\s+          # Отставание\n",
    "            (=?\\s*\\d+)\\s*               # Место\n",
    "            (.*)                        # Примечание (если есть)\n",
    "        '''\n",
    "        match = re.search(pattern, line, re.VERBOSE | re.IGNORECASE)\n",
    "        return match.groups() if match else False\n",
    "    \n",
    "    def parse_all(self):\n",
    "        \"\"\"Парсит все данные и возвращает три Spark DataFrame\"\"\"\n",
    "        if not self.soup:\n",
    "            return None, None, None\n",
    "        try:\n",
    "            df_events = self.parse_events()\n",
    "            df_distances = self.parse_distances()\n",
    "            df_results = self.parse_results()\n",
    "            return df_events, df_distances, df_results\n",
    "        except Exception as e:\n",
    "            print(\"parse_all error: \", e)\n",
    "            return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VRNFSO_html_processor:\n",
    "\n",
    "#     def __init__(self, url_date, html_page):\n",
    "#         self.url_date = url_date\n",
    "#         self.soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "#     def get_date(self):\n",
    "#         year = self.url_date[:4]\n",
    "#         month = self.url_date[4:6]\n",
    "#         day = self.url_date[6:8]\n",
    "#         return f\"{day}.{month}.{year}\"\n",
    "\n",
    "#     def parse_events(self):\n",
    "#         \"\"\"Парсит название соревнования, дату и город.\"\"\"\n",
    "#         if not self.soup:\n",
    "#             return None\n",
    "        \n",
    "#         try:\n",
    "#             title_text = self.soup.find(\"h1\").get_text(separator=\" \").strip()\n",
    "#             date = self.get_date()\n",
    "\n",
    "#             parts = title_text.split(date) if date else [title_text]\n",
    "                        \n",
    "#             # event_name = parts[0].strip()\n",
    "#             event_name = clean_text(parts[0])\n",
    "\n",
    "#             if \"эстафета\" in event_name.lower():\n",
    "#                 print(\"Пропуск эстафеты\")\n",
    "#                 return None\n",
    "\n",
    "#             city_match = re.search(r\"г\\.\\s*([\\w\\s]+)\\n\", parts[1]) if len(parts) > 1 else None\n",
    "#             city = city_match.group(1) if city_match else None\n",
    "\n",
    "#             df_events = pd.DataFrame([[event_name, date, city]], columns=[\"Название старта\", \"Дата\", \"Город\"])\n",
    "\n",
    "#             return df_events\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(\"parse_events error: \", e)\n",
    "#             return None\n",
    "\n",
    "#     def parse_distances(self):\n",
    "#         \"\"\"Парсит информацию о дистанциях.\"\"\"\n",
    "#         if not self.soup:\n",
    "#             return None\n",
    "#         distances = []\n",
    "#         date = self.get_date()\n",
    "\n",
    "#         try:\n",
    "#             for h2 in self.soup.find_all(\"h2\"):\n",
    "#                 text = h2.get_text()\n",
    "#                 match = re.match(r\"(.+),\\s*(\\d+)\\s*КП,\\s*([\\d.,]+)\\s*(км|м)\", text)\n",
    "#                 # print(match)\n",
    "#                 if match:\n",
    "#                     group, kp, length, _ = match.groups()\n",
    "#                     length = float(length.replace(\",\", \".\"))\n",
    "#                     distances.append([date, group, int(kp), length])\n",
    "\n",
    "#             df_distances = pd.DataFrame(distances, columns=[\"Дата\", \"Группа\", \"КП\", \"Длина\"])\n",
    "            \n",
    "#             return df_distances\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(\"parse_distances error: \", e)\n",
    "#             return None\n",
    "\n",
    "#     def parse_results(self):\n",
    "#         \"\"\"Парсит результаты соревнований.\"\"\"\n",
    "#         if not self.soup:\n",
    "#             return None\n",
    "#         categories = self.soup.find_all('h2')\n",
    "#         df_results = pd.DataFrame()\n",
    "#         date = self.get_date()\n",
    "\n",
    "#         try:\n",
    "#             for category in categories:\n",
    "#                 group = category.get_text(strip=True).split(',')[0]\n",
    "#                 pre_tag = category.find_next('pre')\n",
    "\n",
    "#                 if not pre_tag:\n",
    "#                     continue\n",
    "\n",
    "#                 lines = pre_tag.get_text().split('\\n')\n",
    "#                 data_rows = []\n",
    "\n",
    "#                 for line in lines:\n",
    "#                     fields = self.results_regex_parser(line)\n",
    "#                     if fields:\n",
    "#                         place = fields[8].replace('=', '').strip()\n",
    "#                         data_rows.append((\n",
    "#                             date, group, int(fields[0]), fields[1], fields[2], fields[3],\n",
    "#                             int(fields[4]), int(fields[5]), fields[6], fields[7], int(place)\n",
    "#                         ))\n",
    "\n",
    "#                 columns = [\"Дата\", \"Группа\", \"№п/п\", \"Фамилия, имя\", \"Коллектив\", \"Квал\", \"Номер\", \"ГР\", \"Результат\", \"Отставание\", \"Место\"]\n",
    "#                 df_group = pd.DataFrame(data_rows, columns=columns)\n",
    "#                 df_results = pd.concat([df_results, df_group], ignore_index=True)\n",
    "\n",
    "#             return df_results\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(\"parse_results error: \", e)\n",
    "#             return None\n",
    "    \n",
    "\n",
    "#     @staticmethod\n",
    "#     def results_regex_parser(line):\n",
    "#         \"\"\"Парсит строку с результатами.\"\"\"\n",
    "#         pattern = r'''\n",
    "#             ^\\s*(\\d+)\\s+                # №п/п\n",
    "#             ([А-ЯЁ][а-яё-]+\\s[А-ЯЁ][а-яё-]+)\\s+  # Фамилия и имя\n",
    "#             (.*?)\\s{2,}                 # Коллектив\n",
    "#             ([А-Яa-zIЮМСК]+)?\\s*        # Квал (может отсутствовать)\n",
    "#             (\\d+)\\s+                    # Номер\n",
    "#             (\\d{4})\\s+                  # Год рождения\n",
    "#             (\\d{2}:\\d{2}:\\d{2})\\s+      # Результат\n",
    "#             (\\+\\d{2}:\\d{2})\\s+          # Отставание\n",
    "#             (=?\\s*\\d+)\\s*               # Место\n",
    "#             (.*)                        # Примечание (если есть)\n",
    "#         '''\n",
    "#         match = re.search(pattern, line, re.VERBOSE | re.IGNORECASE)\n",
    "#         return match.groups() if match else False\n",
    "\n",
    "#     def parse_all(self):\n",
    "#         \"\"\"Парсит все данные (название соревнования, дистанции, результаты).\"\"\"\n",
    "#         if not self.soup:\n",
    "#             return None, None, None\n",
    "#         try:\n",
    "#             df_events = self.parse_events()\n",
    "#             df_distances = self.parse_distances()\n",
    "#             df_results = self.parse_results()\n",
    "#             return df_events, df_distances, df_results\n",
    "#         except Exception as e:\n",
    "#             print(\"parse_all error: \", e)\n",
    "#             return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240407', '20240412', '20240413', '20240414', '20240420', '20240421', '20240428', '20240429', '20240430', '20240509', '20240510', '20240511', '20240512', '20240518', '20240521', '20240522', '20240525', '20240526', '20240602', '20240608', '20240609', '20240721', '20240907', '20240908', '20240913', '20240914', '20240915', '20240928', '20240929', '20241006', '20241012', '20241013', '20241019', '20241026', '20241027', '20241102', '20241103', '20241117', '20241201']\n"
     ]
    }
   ],
   "source": [
    "bucket_obj_list = [obj.object_name for obj in client.list_objects(bucket_html)]\n",
    "print(bucket_obj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропуск эстафеты\n",
      "Пропуск эстафеты\n",
      "Пропуск эстафеты\n",
      "Пропуск эстафеты\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16041/2604435349.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  pdf_distances = pd.concat(distances_list, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "events_list = []\n",
    "distances_list = []\n",
    "results_list = []\n",
    "\n",
    "for obj in bucket_obj_list:\n",
    "    response = client.get_object(bucket_html, obj)\n",
    "    html_str = response.read().decode('windows-1251')\n",
    "    parser = VRNFSO_html_processor(obj, html_str)\n",
    "    df_events, df_distances, df_results = parser.parse_all()\n",
    "    \n",
    "    if df_events is not None:\n",
    "        events_list.append(df_events)\n",
    "    if df_distances is not None:\n",
    "        distances_list.append(df_distances)\n",
    "    if df_results is not None:\n",
    "        results_list.append(df_results)\n",
    "\n",
    "# Объединяем таблицы pandas\n",
    "pdf_events = pd.concat(events_list, ignore_index=True)\n",
    "pdf_distances = pd.concat(distances_list, ignore_index=True)\n",
    "pdf_results = pd.concat(results_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_events = spark.createDataFrame(pdf_events)\n",
    "sdf_distances = spark.createDataFrame(pdf_distances)\n",
    "sdf_results = spark.createDataFrame(pdf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_events_cleaned = sdf_events.fillna(\"Воронеж\", subset=[\"Город\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/24 21:56:52 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf_events_cleaned.write.mode(\"overwrite\").parquet(\"s3a://processed-data/events/\")\n",
    "sdf_distances.write.mode(\"overwrite\").parquet(\"s3a://processed-data/distances/\")\n",
    "sdf_results.write.mode(\"overwrite\").parquet(\"s3a://processed-data/results/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Название старта: string (nullable = true)\n",
      " |-- Дата: string (nullable = true)\n",
      " |-- Город: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Дата: string (nullable = true)\n",
      " |-- Группа: string (nullable = true)\n",
      " |-- КП: long (nullable = true)\n",
      " |-- Длина: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Дата: string (nullable = true)\n",
      " |-- Группа: string (nullable = true)\n",
      " |-- №п/п: long (nullable = true)\n",
      " |-- Фамилия, имя: string (nullable = true)\n",
      " |-- Коллектив: string (nullable = true)\n",
      " |-- Квал: string (nullable = true)\n",
      " |-- Номер: long (nullable = true)\n",
      " |-- ГР: long (nullable = true)\n",
      " |-- Результат: string (nullable = true)\n",
      " |-- Отставание: string (nullable = true)\n",
      " |-- Место: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_events_loaded = spark.read.parquet(\"s3a://processed-data/events/\")\n",
    "sdf_distances_loaded = spark.read.parquet(\"s3a://processed-data/distances/\")\n",
    "sdf_results_loaded = spark.read.parquet(\"s3a://processed-data/results/\")\n",
    "\n",
    "# sdf_events_cleaned = sdf_events_loaded.fillna(\"Воронеж\", subset=[\"Город\"])\n",
    "\n",
    "# sdf_events_cleaned.write.mode(\"overwrite\").parquet(\"s3a://processed-data/events/\")\n",
    "\n",
    "sdf_events_loaded.printSchema()\n",
    "sdf_distances_loaded.printSchema()\n",
    "sdf_results_loaded.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = \"jdbc:postgresql://127.0.0.1:5433/postgres\"\n",
    "\n",
    "db_properties = {\n",
    "    \"user\": postgres_user,\n",
    "    \"password\": postgres_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Запись в PostgreSQL\n",
    "sdf_events_loaded.write.mode(\"overwrite\").jdbc(db_url, \"public.events\", properties=db_properties)\n",
    "sdf_distances_loaded.write.mode(\"overwrite\").jdbc(db_url, \"public.distances\", properties=db_properties)\n",
    "sdf_results_loaded.write.mode(\"overwrite\").jdbc(db_url, \"public.results\", properties=db_properties)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
